{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have added 12 tokens for gpt2\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import argparse\n",
    "import logging\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "import pandas\n",
    "import tqdm\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "import copy\n",
    "import csv\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, Sampler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from models import SSTModel, MaskedNLLCriterion\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, GPT2Config\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "from data import add_special_tokens_, LengthSampler, MADataset, handle_raw_keywords\n",
    "import pandas as pd\n",
    "ATTR_TO_SPECIAL_TOKEN = {'pad_token': '<pad>'}\n",
    "\n",
    "# gpt-2\n",
    "EOS_ID = 50256\n",
    "\n",
    "pos_token = \"<pos>\"\n",
    "neg_token = \"<neg>\"\n",
    "neutral_token = \"<neu>\"\n",
    "\n",
    "genre_tokens_map = {\"Comedy\": \"<Comedy>\" , \"Romance\": \"<Romance>\", \"Action\":\"<Action>\", \"Thriller\":\"<Thriller>\", \"Horror\":\"<Horror>\", \"Crime\":\"<Crime>\", \"Science\":\"<Science>\", \"Fantasy\":\"<Fantasy>\"}\n",
    "genre_tokens = [\"<Comedy>\" , \"<Romance>\", \"<Action>\", \"<Thriller>\", \"<Horror>\", \"<Crime>\", \"<Science>\", \"<Fantasy>\"]\n",
    "\n",
    "domain_token = \"<summary>\"\n",
    "\n",
    "SMALL_CONST = 1e-15\n",
    "\n",
    "train_data = pd.read_csv(\"movie_data.csv\")\n",
    "keywords_set = pd.read_csv(\"keywords.csv\")\n",
    "keywords_set['ordered_words'] = keywords_set['ordered_words'].apply(lambda x:handle_raw_keywords(x,3))\n",
    "keywords_set = keywords_set.loc[:,['movie_id','ordered_words']]\n",
    "keywords_set = keywords_set.rename(columns={'movie_id':'Movie ID',\"ordered_words\":\"Keywords\"})\n",
    "train_data = pd.merge(train_data, keywords_set, left_on=\"Movie ID\", right_on=\"Movie ID\", how=\"inner\")\n",
    "train_data = train_data.iloc[:, 1:5]\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "config = GPT2Config.from_pretrained(\"gpt2\")\n",
    "lm_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# add special token\n",
    "add_special_tokens_(lm_model, tokenizer, ATTR_TO_SPECIAL_TOKEN)\n",
    "num_added_toks = tokenizer.add_tokens([pos_token, neg_token, neutral_token, domain_token])\n",
    "num_added_toks += tokenizer.add_tokens(genre_tokens)\n",
    "print('We have added', num_added_toks, 'tokens for gpt2')\n",
    "lm_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "gpt2_padding_value = tokenizer.convert_tokens_to_ids(\"<pad>\")\n",
    "\n",
    "train_data['Movie Category'] = train_data['Movie Category'].apply(lambda x:tokenizer.encode(genre_tokens_map.get(x)))\n",
    "train_data['Summary'] = train_data['Summary'].apply(lambda x:tokenizer.encode(x[:1024]))\n",
    "train_data['Sentiment'] = train_data['Sentiment'].apply(lambda x:tokenizer.encode(pos_token if x==\"POSITIVE\" else neg_token))\n",
    "train_data['Keywords'] = train_data['Keywords'].apply(lambda x:tokenizer.encode(x))\n",
    "train_data = train_data.drop(8653)\n",
    "train_data = train_data.reset_index()\n",
    "train_data = train_data.drop('index', axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\limoy\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 7\n",
    "ACC_STEPS = 2\n",
    "WEIGHT_DECAY = 0.0\n",
    "LEARNING_RATE = 5e-5\n",
    "ADAM_EPSILON = 1e-8\n",
    "BAYES=False\n",
    "BATCH_SIZE=2\n",
    "device='cuda'\n",
    "train_dataset = MADataset(train_data,gpt2_padding_value,device=device)\n",
    "train_sampler = LengthSampler(train_data,BATCH_SIZE,shuffle=True)\n",
    "train_loader = DataLoader(dataset=train_dataset, sampler=train_sampler, collate_fn=train_dataset.collate,drop_last=True)\n",
    "\n",
    "sst_model = SSTModel(lm_model=lm_model,config=config,use_label_only=False)\n",
    "sst_model.to(device)\n",
    "\n",
    "num_train_steps = len(train_loader) // ACC_STEPS * NUM_EPOCHS\n",
    "criterion = MaskedNLLCriterion()\n",
    "\n",
    "parameters = list(filter(lambda p: p[1].requires_grad, list(sst_model.named_parameters())))\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "        {\n",
    "            'params': [p for n, p in parameters if not any(nd in n for nd in no_decay)],\n",
    "            'weight_decay': WEIGHT_DECAY,\n",
    "        },\n",
    "        {'params': [p for n, p in parameters if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=LEARNING_RATE, eps=ADAM_EPSILON)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,num_warmup_steps=0, num_training_steps=num_train_steps)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1, global_steps: 0,  training batch loss: 0.214516; lr: 0.000050\n",
      "batch: 201, global_steps: 100,  training batch loss: 0.900006; lr: 0.000050\n",
      "batch: 401, global_steps: 200,  training batch loss: 0.006623; lr: 0.000050\n",
      "batch: 601, global_steps: 300,  training batch loss: 0.005333; lr: 0.000049\n",
      "batch: 801, global_steps: 400,  training batch loss: 0.001554; lr: 0.000049\n",
      "batch: 1001, global_steps: 500,  training batch loss: 0.000307; lr: 0.000049\n",
      "batch: 1201, global_steps: 600,  training batch loss: 0.000239; lr: 0.000049\n",
      "batch: 1401, global_steps: 700,  training batch loss: 0.000047; lr: 0.000049\n",
      "batch: 1601, global_steps: 800,  training batch loss: 0.000105; lr: 0.000048\n",
      "batch: 1801, global_steps: 900,  training batch loss: 0.000107; lr: 0.000048\n",
      "batch: 2001, global_steps: 1000,  training batch loss: 0.000100; lr: 0.000048\n",
      "batch: 2201, global_steps: 1100,  training batch loss: 0.000067; lr: 0.000048\n",
      "batch: 2401, global_steps: 1200,  training batch loss: 0.000076; lr: 0.000048\n",
      "batch: 2601, global_steps: 1300,  training batch loss: 0.000024; lr: 0.000047\n",
      "batch: 2801, global_steps: 1400,  training batch loss: 0.000016; lr: 0.000047\n",
      "batch: 3001, global_steps: 1500,  training batch loss: 0.000008; lr: 0.000047\n",
      "batch: 3201, global_steps: 1600,  training batch loss: 0.000006; lr: 0.000047\n",
      "batch: 3401, global_steps: 1700,  training batch loss: 0.000004; lr: 0.000047\n",
      "batch: 3601, global_steps: 1800,  training batch loss: 0.000035; lr: 0.000046\n",
      "batch: 3801, global_steps: 1900,  training batch loss: 0.000018; lr: 0.000046\n",
      "batch: 4001, global_steps: 2000,  training batch loss: 0.000007; lr: 0.000046\n",
      "batch: 4201, global_steps: 2100,  training batch loss: 0.000101; lr: 0.000046\n",
      "batch: 4401, global_steps: 2200,  training batch loss: 0.000065; lr: 0.000046\n",
      "batch: 4601, global_steps: 2300,  training batch loss: 0.000167; lr: 0.000045\n",
      "batch: 4801, global_steps: 2400,  training batch loss: 0.000022; lr: 0.000045\n",
      "batch: 5001, global_steps: 2500,  training batch loss: 0.000060; lr: 0.000045\n",
      "batch: 5201, global_steps: 2600,  training batch loss: 0.000016; lr: 0.000045\n",
      "batch: 5401, global_steps: 2700,  training batch loss: 0.000017; lr: 0.000045\n",
      "batch: 5601, global_steps: 2800,  training batch loss: 0.000043; lr: 0.000044\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 290.00 MiB (GPU 0; 8.00 GiB total capacity; 6.75 GiB already allocated; 0 bytes free; 7.11 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 27\u001B[0m\n\u001B[0;32m     25\u001B[0m     loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m domain_loss\n\u001B[0;32m     26\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 27\u001B[0m     out \u001B[38;5;241m=\u001B[39m \u001B[43msst_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdomain_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mD\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkeywords_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mK\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msentiment_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mS\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgenre_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mG\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdomain_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mDM\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkeyword_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mKM\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msentiment_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mSM\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgenre_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mGM\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mM\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     28\u001B[0m     out_logit \u001B[38;5;241m=\u001B[39m softmax(out\u001B[38;5;241m.\u001B[39mlogits)\n\u001B[0;32m     29\u001B[0m     loss \u001B[38;5;241m=\u001B[39m criterion(out_logit, X\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m), M\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m))\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\OneDrive\\Desktop\\CSCI544_project_2(1)\\CSCI544_project\\models.py:129\u001B[0m, in \u001B[0;36mSSTModel.forward\u001B[1;34m(self, input_ids, domain_id, keywords_id, sentiment_id, genre_id, domain_mask, keyword_mask, sentiment_mask, genre_mask, mask, BAYES)\u001B[0m\n\u001B[0;32m    126\u001B[0m position_ids \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39marange(\u001B[38;5;241m0\u001B[39m, input_shape[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m], dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mlong, device\u001B[38;5;241m=\u001B[39mdevice)\n\u001B[0;32m    127\u001B[0m position_ids \u001B[38;5;241m=\u001B[39m position_ids\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m0\u001B[39m)\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, input_shape[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m])\n\u001B[1;32m--> 129\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlm_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtransfered_past\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    130\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m BAYES:\n\u001B[0;32m    131\u001B[0m     domain_out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlm_model(input_ids, past_key_values\u001B[38;5;241m=\u001B[39mdomain_transfered_past, position_ids\u001B[38;5;241m=\u001B[39mposition_ids)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:1065\u001B[0m, in \u001B[0;36mGPT2LMHeadModel.forward\u001B[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[0;32m   1062\u001B[0m     torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mset_device(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransformer\u001B[38;5;241m.\u001B[39mfirst_device)\n\u001B[0;32m   1063\u001B[0m     hidden_states \u001B[38;5;241m=\u001B[39m hidden_states\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlm_head\u001B[38;5;241m.\u001B[39mweight\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m-> 1065\u001B[0m lm_logits \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlm_head\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1067\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1068\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m labels \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   1069\u001B[0m     \u001B[38;5;66;03m# Shift so that tokens < n predict n\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001B[0m, in \u001B[0;36mLinear.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 290.00 MiB (GPU 0; 8.00 GiB total capacity; 6.75 GiB already allocated; 0 bytes free; 7.11 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "sst_model.train()\n",
    "sst_model.lm_model.eval()\n",
    "\n",
    "sst_model.zero_grad()\n",
    "global_steps = -1\n",
    "softmax = nn.LogSoftmax(dim=-1)\n",
    "for epochs in range(NUM_EPOCHS):\n",
    "    total_loss = 0\n",
    "    total_ce_loss, total_kl_loss = 0, 0\n",
    "    total_ppl = 0\n",
    "\n",
    "    for batch_num, train_batch in enumerate(train_loader):\n",
    "        torch.cuda.empty_cache()\n",
    "        D = torch.tensor([tokenizer.convert_tokens_to_ids(domain_token)] * BATCH_SIZE).unsqueeze(1).to(device)\n",
    "        DM = (D != gpt2_padding_value).byte()\n",
    "\n",
    "        X, M, S, K, G, KM, SM, GM = train_batch\n",
    "\n",
    "        if BAYES:\n",
    "            out, domain_out = sst_model(input_ids=X, domain_id=D, keywords_id=K, sentiment_id=S, genre_id=G, domain_mask=DM, keyword_mask=KM, sentiment_mask=SM, genre_mask=GM, mask=M, BAYES=BAYES)\n",
    "            out_logit = softmax(out.logits)\n",
    "            domain_out_logit = softmax(domain_out.logits)\n",
    "            loss = criterion(out_logit, X.unsqueeze(-1), M.unsqueeze(-1))\n",
    "            domain_loss = criterion(domain_out_logit, X.unsqueeze(-1), M.unsqueeze(-1))\n",
    "            loss += domain_loss\n",
    "        else:\n",
    "            out = sst_model(input_ids=X, domain_id=D, keywords_id=K, sentiment_id=S, genre_id=G, domain_mask=DM, keyword_mask=KM, sentiment_mask=SM, genre_mask=GM, mask=M)\n",
    "            out_logit = softmax(out.logits)\n",
    "            loss = criterion(out_logit, X.unsqueeze(-1), M.unsqueeze(-1))\n",
    "\n",
    "        loss = loss / 2\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        if (batch_num + 1) % 2 == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(sst_model.parameters(), 1)\n",
    "\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            sst_model.zero_grad()\n",
    "            global_steps += 1\n",
    "\n",
    "            if global_steps % 100 == 0 and global_steps >= 0:\n",
    "                print(\"batch: %d, global_steps: %d,  training batch loss: %f; lr: %f\" % (batch_num, global_steps, total_loss / 500, scheduler.get_last_lr()[0]))\n",
    "                total_loss = 0\n",
    "\n",
    "    model_path = 'model_at_epoch_%s.pt' % str(epochs)\n",
    "    tokenizer_folder = 'tokenizer_at_epoch_%s' % str(epochs)\n",
    "    if not os.path.exists(tokenizer_folder):\n",
    "        os.makedirs(tokenizer_folder)\n",
    "    torch.save(sst_model.state_dict(), model_path)\n",
    "    tokenizer.save_pretrained(tokenizer_folder)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "for i in range(len(train_data)):\n",
    "    if len(train_data.Keywords[i]) == 0:\n",
    "        print(i)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "   a  b\n0  1  2\n1  2  3\n2  4  5",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>a</th>\n      <th>b</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame()\n",
    "df['a'] = [1,2,3,4]\n",
    "df['b'] = [2,3,4,5]\n",
    "df = df.drop(2)\n",
    "df = df.reset_index()\n",
    "df = df.drop('index',axis=1)\n",
    "df"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
